\documentclass[11pt]{article}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{float}
\usepackage{listings}

\usepackage[a4paper, margin=1in]{geometry}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}


\newcommand{\Submitted}[1]{Submitted to #1}

\begin{document}

\title{Introduction to Reinforcement Learning }


\author{Syed Mustafa \\ Habib University\\ sm06554@st.habib.edu.pk\\\Submitted{Shahid Shaikh}}

\maketitle

\section{Planning}
Topics tested in this assignment:
\begin{enumerate}
    \item Optimal Policy and Optimal function.
    \item Chapter 3, 4 and 5. 
\end{enumerate}

\section*{Explanation of GridWorld}
This section is to explain the MDP for question 1 and 2.\\
This is a grid representation of a finite Markov Decision Process (MDP). Each cell
is a state of the MDP. Four actions are possible at a given state, i.e. \textbf{north, south,
east, west}. Actions that would cause the agent to get of the grid leave its location unchanged.
\newpage
\section*{Question 1}
The optimal value function v\(*\) for the GridWorld is generated by the code GridWorld 3 5.py
which is available on Canvas. Write a program which takes the value function generated by
the above code as input and generates the corresponding optimal policy.

\begin{figure}[H]
    \begin{center}
        \includegraphics*[scale = 0.5]{Policy Iterattion.jpg}
        \caption{Policy Iteration Psuedocode}            
    \end{center}
\end{figure}

The given code has already implemented the policy evaluation. Now to obtain a optimal policy we must implement policy improvement.
\\
The code for this code is provided below:
\\
\lstinputlisting[language=python]{question1.py}

\newpage
\section*{Question 2}
Starting from the code GridWorld 3 2.py, which is available on Canvas, implement the
complete value iteration algorithm to generate the optimal value function $v*$ and an optimal
policy \(\pi\)\(*\).
\begin{figure}[H]
    \begin{center}
        \includegraphics*[scale = 1]{Value Iteration.jpg}
        \caption{Policy Iteration Psuedocode}            
    \end{center}
\end{figure}
\lstinputlisting[language=python]{question1.py}


\newpage



\section*{Google Colab Notebook}
Please fnd the link to the google colab notebook: \\
https://colab.research.google.com/drive/1SUZGc8QwLwSf7NQFFGH8O0Q4ZLBHoh-6?usp=sharing



\begin{thebibliography}{00}
\bibitem{b1} G. Eason, B. Noble, and I. N. Sneddon, ``On certain integrals of Lipschitz-Hankel type involving products of Bessel functions,'' Phil. Trans. Roy. Soc. London, vol. A247, pp. 529--551, April 1955.
\bibitem{b2} J. Clerk Maxwell, A Treatise on Electricity and Magnetism, 3rd ed., vol. 2. Oxford: Clarendon, 1892, pp.68--73.
\bibitem{b3} I. S. Jacobs and C. P. Bean, ``Fine particles, thin films and exchange anisotropy,'' in Magnetism, vol. III, G. T. Rado and H. Suhl, Eds. New York: Academic, 1963, pp. 271--350.
\bibitem{b4} K. Elissa, ``Title of paper if known,'' unpublished.
\bibitem{b5} R. Nicole, ``Title of paper with only first word capitalized,'' J. Name Stand. Abbrev., in press.
\bibitem{b6} Y. Yorozu, M. Hirano, K. Oka, and Y. Tagawa, ``Electron spectroscopy studies on magneto-optical media and plastic substrate interface,'' IEEE Transl. J. Magn. Japan, vol. 2, pp. 740--741, August 1987 [Digests 9th Annual Conf. Magnetics Japan, p. 301, 1982].
\bibitem{b7} M. Young, The Technical Writer's Handbook. Mill Valley, CA: University Science, 1989.
\end{thebibliography}

\vspace{12pt}
\color{red}
IEEE conference templates contain guidance text for composing and formatting conference papers. Please ensure that all template text is removed from your conference paper prior to submission to the conference. Failure to remove the template text from your paper may result in your paper not being published.

\end{document}
